{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would require following libraries to prepare our data.\n",
    "\n",
    "* The listdir and join function are required to combine multiple text files into one corpus.\n",
    "* The sent_tokenize and word_tokenize would tokenize the reviews into words and sentences.\n",
    "* The RegexpTokenizer can be used to extract special characters and alphanumeric characters from the text.\n",
    "* Stop words are words such as \"and, the\" etc. which are most frequently used in sentences but do not contribute much   to the overall meaning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three variables to start with:\n",
    "\n",
    "* stop_words: we would import stop words frequently used in English language from NLTK and assign it to the variable.\n",
    "* exclude_list: We would tag the part-of-speech of every sentence and would like to exclude Nouns(Singular),    Nouns(Plural) and Proper Nouns from our sentences. \n",
    "* SpeCharword_tokenizer: This ('[A-Za-z0-9]\\w*') indicates that the RegexpTokenizer will only consider words and numbers and ignore special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "exclude_list = ['NN','NNS','NNP']\n",
    "SpeCharword_tokenizer = RegexpTokenizer('[A-Za-z0-9]\\w*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The read_txt_data function below will read every text document and store it in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt_data(filename):\n",
    "    data = []\n",
    "    file = open(filename, \"r\", encoding='utf-8') \n",
    "    for line in file.readlines():\n",
    "        data.append(SpeCharword_tokenizer.tokenize(line.lower().replace('\\n','').replace('<br />','')))\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of 12,500 labeled reviews with positive and negative sentiments each for training. Every review is stored in a seperate text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_folder_pos = \"/Users/rajdesai/Downloads/aclImdb/train/pos\"\n",
    "file_folder_neg = \"/Users/rajdesai/Downloads/aclImdb/train/neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we would take 7000 positive and negative reviews and store them in two different lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positiveFiles = []\n",
    "index = 0\n",
    "for textfile in listdir(file_folder_pos):\n",
    "        if(index <7000):\n",
    "                positiveFiles.extend(read_txt_data(join(file_folder_pos,textfile)))\n",
    "                print(index)\n",
    "        else:\n",
    "                break\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negativeFiles = []\n",
    "index = 0\n",
    "for textfile in listdir(file_folder_neg):\n",
    "        if(index <7000):\n",
    "                negativeFiles.extend(read_txt_data(join(file_folder_neg,textfile)))\n",
    "                print(index)\n",
    "        else:\n",
    "                break\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we would create two lists of test data with 200 reviews each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_folder_pos_test = \"/Users/rajdesai/Downloads/aclImdb/test/pos\"\n",
    "file_folder_neg_test = \"/Users/rajdesai/Downloads/aclImdb/test/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positiveTestFiles = []\n",
    "\n",
    "index = 0\n",
    "for textfile in listdir(file_folder_pos_test ):\n",
    "        if(index <200):\n",
    "                positiveTestFiles.extend(read_txt_data(join(file_folder_pos_test ,textfile)))\n",
    "        else:\n",
    "                break\n",
    "        index = index + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negativeTestFiles = []\n",
    "\n",
    "index = 0\n",
    "for textfile in listdir(file_folder_neg_test):\n",
    "        if(index <200):\n",
    "                negativeTestFiles.extend(read_txt_data(join(file_folder_neg_test,textfile)))\n",
    "        else:\n",
    "                break\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would create a bag of words which would give us a distinct word count of every word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_bag_of_words= {}\n",
    "for line in positiveFiles:\n",
    "        for word in line:\n",
    "                dict_bag_of_words[word] = dict_bag_of_words.get(word, 0)+1\n",
    "\n",
    "for line in negativeFiles:\n",
    "        for word in line:\n",
    "                dict_bag_of_words[word] = dict_bag_of_words.get(word, 0)+1\n",
    "\n",
    "\n",
    "for k,v in sorted(dict_bag_of_words.items(), key=lambda p:p[1]):\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have downloaded two pre-trained models which would help us with feature extraction from : https://github.com/adeshpande3/LSTM-Sentiment-Analysis\n",
    "\n",
    "* The wordsList.npy and wordVectors.npy contain a list of words and its vector representation using the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrdlistmodel = \"/Users/rajdesai/Downloads/models/wordsList.npy\"\n",
    "wrdvecmodel = \"/Users/rajdesai/Downloads/models/wordVectors.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load both the models. Also, the wordsList model is converted to a list. It initially gets loaded as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsList = np.load(wrdlistmodel)\n",
    "wordsList = wordsList.tolist() \n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]\n",
    "wordVectors = np.load(wrdvecmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function would pass each sentence from our reviews compare it with the words present in the models we loaded and assign a vector value.\n",
    "At the end of it each sentence would have a aggregate vector value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector_array(sentence):\n",
    "        \"\"\" SENTENCE TO VECTOR GENERATOR \"\"\"\n",
    "        sentence = SpeCharword_tokenizer.tokenize(sentence.lower().replace('\\n','').replace('<br />',''))\n",
    "        print(sentence)\n",
    "        vect = np.zeros((24, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "        indx = 0\n",
    "        for word in sentence:\n",
    "                try:\n",
    "                        vect[0][indx] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                        vect[0][indx] = 399999 #Vector for unkown words\n",
    "                indx  = indx  + 1\n",
    "                ip_labels.append([1,0])\n",
    "                if indx >= MAX_SENTENCE_LENGTH:\n",
    "                        break\n",
    "        return vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function iterates through our review files and creates a matrix which would be a numpy array.\n",
    "\n",
    "The matrix would consists of an index number which represents the review id, the vector value of the document and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_count = len(positiveFiles) + len(negativeFiles)\n",
    "ids = np.zeros((file_count, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "file_indx = 0\n",
    "ip_labels = []\n",
    "for line in positiveFiles:\n",
    "        indx = 0\n",
    "        for word in line:\n",
    "                print(file_indx)\n",
    "                print(indx)\n",
    "                try:\n",
    "                        ids[file_indx][indx] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                        ids[file_indx][indx] = 399999 #Vector for unkown words\n",
    "                indx  = indx  + 1\n",
    "                ip_labels.append([1,0])\n",
    "                if indx >= MAX_SENTENCE_LENGTH:\n",
    "                        break\n",
    "        file_indx = file_indx  + 1\n",
    "        \n",
    "for line in negativeFiles:\n",
    "        indx = 0\n",
    "        for word in line:\n",
    "                print(file_indx)\n",
    "                print(indx)\n",
    "                try:\n",
    "                        ids[file_indx][indx] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                        ids[file_indx][indx] = 399999  #Vector for unkown words\n",
    "                indx  = indx  + 1\n",
    "                ip_labels.append([0,1])\n",
    "                if indx >= MAX_SENTENCE_LENGTH:\n",
    "                        break\n",
    "        file_indx = file_indx  + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the matrix, i.e the array we receive at the end of last iteration as idsMatrix1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/Users/rajdesai/Downloads/models/idsMatrix1', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_id = np.load('/Users/rajdesai/Downloads/models/idsMatrix1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in load_id[0]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we would get a test matrix which would be an array of file index numbers and test tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file_count = len(positiveTestFiles) + len(negativeTestFiles)\n",
    "test_ids = np.zeros((test_file_count, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "file_indx = 0\n",
    "test_labels = []\n",
    "for line in positiveTestFiles:\n",
    "        indx = 0\n",
    "        for word in line:\n",
    "                print(file_indx)\n",
    "                print(indx)\n",
    "                try:\n",
    "                        test_ids[file_indx][indx] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                        test_ids[file_indx][indx] = 399999 #Vector for unkown words\n",
    "                indx  = indx  + 1\n",
    "                test_labels.append([1,0])\n",
    "                if indx >= MAX_SENTENCE_LENGTH:\n",
    "                        break\n",
    "        file_indx = file_indx  + 1\n",
    "        \n",
    "for line in negativeTestFiles:\n",
    "        indx = 0\n",
    "        for word in line:\n",
    "                print(file_indx)\n",
    "                print(indx)\n",
    "                try:\n",
    "                        test_ids[file_indx][indx] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                        test_ids[file_indx][indx] = 399999  #Vector for unkown words\n",
    "                indx  = indx  + 1\n",
    "                test_labels.append([0,1])\n",
    "                if indx >= MAX_SENTENCE_LENGTH:\n",
    "                        break\n",
    "        file_indx = file_indx  + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the vector matrix which would be our input to our LSTM model ready, we start the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we would define our training parameters:\n",
    "* batchSize- Number of samples per iteration.\n",
    "* lstmUnits- Number of hidden states.\n",
    "\n",
    "After which we initialize a tensorflow session.\n",
    "\n",
    "* we define a weight and bias matrix.\n",
    "\n",
    "* Softmax function is used in the output cell.\n",
    "\n",
    "* Adam optimizer is used to minimize the loss.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000\n",
    "numDimensions = 300\n",
    "\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, MAX_SENTENCE_LENGTH])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, MAX_SENTENCE_LENGTH, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32)) \n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrain(batchSize);\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 100 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"/Users/rajdesai/Downloads/models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()\n",
    "\n",
    "\n",
    "iterations = 20\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTest(24);\n",
    "    print(\"Accuracy :\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an input text, which is a sample review to test our model.\n",
    "\n",
    "We generate the vector array of the review and pass it through our model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_text = \"This is really a new low in entertainment. Even though there are a lot worse movies out.<br /><br />In the Gangster / Drug scene genre it is hard to have a convincing storyline (this movies does not, i mean Sebastians motives for example couldn't be more far fetched and worn out cliché.) Then you would also need a setting of character relationships that is believable (this movie does not.) <br /><br />Sure Tristan is drawn away from his family but why was that again? what's the deal with his father again that he has to ask permission to go out at his age? interesting picture though to ask about the lack and need of rebellious behavior of kids in upper class family. But this movie does not go in this direction. Even though there would be the potential judging by the random Backflashes. Wasn't he already down and out, why does he do it again? <br /><br />So there are some interesting questions brought up here for a solid socially critic drama (but then again, this movie is just not, because of focusing on cool production techniques and special effects an not giving the characters a moment to reflect and most of all forcing the story along the path where they want it to be and not paying attention to let the story breath and naturally evolve.) <br /><br />It wants to be a drama to not glorify abuse of substances and violence (would be political incorrect these days, wouldn't it?) but on the other hand it is nothing more then a cheap action movie (like there are so so many out there) with an average set of actors and a Vinnie Jones who is managing to not totally ruin what's left of his reputation by doing what he always does.<br /><br />So all in all i .. just ... can't recommend it.<br /><br />1 for Vinnie and 2 for the editing.\"\n",
    "ip_vect = get_vector_array(ip_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = sess.run(prediction, {input_data: ip_vect})[0]\n",
    "\n",
    "if (predict[0] > predict[1]):\n",
    "        print(\"Positive\")\n",
    "else:\n",
    "        print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
